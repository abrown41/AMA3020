\section{\href{https://doi.org/10.3847/1538-4357/ae1ba5}{Transformers for Stratified Spectropolarimetric Inversion: Proof of Concept}}

\subsection*{Supervisor: Dr Ryan Campbell}
Solar spectropolarimetric inversion—inferring atmospheric 
conditions from the Stokes vector—is a key diagnostic tool 
for understanding solar magnetism, but traditional inversion 
methods are computationally expensive and sensitive to local 
minima. Advances in artificial intelligence offer faster 
solutions, but are often restricted to shallow models or a few 
spectral lines. We present a proof-of-concept study using a 
transformer machine learning model for multiline, full-Stokes 
inversion, to infer stratified parameters from synthetic spectra 
produced from 3D magnetohydrodynamic simulations. We synthesize a 
large set of Stokes vectors using forward modeling across 15 spectral 
lines spanning the deep photosphere toward the chromosphere. The model 
maps full-Stokes input to temperature, magnetic field strength, 
inclination, azimuth (encoded as $\sin 2\phi, \cos 2\phi$), and line-of-sight velocity as 
a function of optical depth. The transformer incorporates an attention 
mechanism that allows the model to focus on the most informative 
regions of the spectrum for each inferred parameter, and uses positional 
embedding to encode wavelength and depth order. We benchmark it against 
a multilayer perceptron (MLP), test robustness to noise, and assess 
generalization. The transformer outperforms the MLP, especially in 
the higher layers and for magnetic parameters, yielding higher 
correlations and more regularized stratifications. The model retains 
strong performance across a range of noise levels typical for real 
observations, with magnetic parameter inference degrading predictably 
while temperature and velocity remain stable. We explore attention maps, 
linking the transformer’s learned behaviour to 
line-formation physics.
